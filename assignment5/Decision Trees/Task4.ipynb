{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8e7732",
   "metadata": {},
   "source": [
    "# Part-II: Decision Trees \n",
    "\n",
    "Objective: \n",
    "To implement Decision Tree classifiers and understand their structure, splits, \n",
    "and overfitting characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629c514",
   "metadata": {},
   "source": [
    "### 1️⃣ What is **Entropy** and **Information Gain**?\n",
    "\n",
    "* **Entropy** is a measure of impurity or randomness in the dataset. It quantifies the uncertainty involved in predicting the class of a given data point.\n",
    "\n",
    "  $$\n",
    "  \\text{Entropy}(S) = -\\sum p_i \\log_2(p_i)\n",
    "  $$\n",
    "\n",
    "  where $p_i$ is the proportion of examples belonging to class $i$.\n",
    "\n",
    "* **Information Gain** is the reduction in entropy after a dataset is split on an attribute. It helps decide which feature to split on at each node in a decision tree.\n",
    "\n",
    "  $$\n",
    "  \\text{Information Gain} = \\text{Entropy}(parent) - \\sum \\frac{n_i}{n} \\cdot \\text{Entropy}(child_i)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Difference Between **Gini Index** and **Entropy**\n",
    "\n",
    "| Criteria       | Gini Index                               | Entropy                              |\n",
    "| -------------- | ---------------------------------------- | ------------------------------------ |\n",
    "| Formula        | $1 - \\sum p_i^2$                         | $-\\sum p_i \\log_2(p_i)$              |\n",
    "| Computation    | Faster and simpler                       | More complex (involves log)          |\n",
    "| Range          | \\[0, 0.5] for binary classification      | \\[0, 1]                              |\n",
    "| Splitting Bias | Tends to isolate the most frequent class | More sensitive to class distribution |\n",
    "---\n",
    "\n",
    "### 3️⃣ How Can a **Decision Tree Overfit**? How Can This Be Avoided?\n",
    "\n",
    "* **Overfitting** occurs when a decision tree becomes too complex and starts capturing noise in the training data, leading to poor generalization on unseen data.\n",
    "\n",
    "#### Causes of Overfitting:\n",
    "\n",
    "* Very deep trees with many branches\n",
    "* Not enough training data\n",
    "* Splitting until each leaf is pure (no pruning)\n",
    "\n",
    "#### Ways to Prevent Overfitting:\n",
    "\n",
    "* **Pruning**: Remove branches that have little importance.\n",
    "* **Set `max_depth`**: Limit the depth of the tree.\n",
    "* **Set `min_samples_split` or `min_samples_leaf`**: Control the minimum number of samples to split a node.\n",
    "* **Use ensemble methods**: Like **Random Forest** or **Gradient Boosted Trees**.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want these answers saved as a separate Markdown file or combined with previous README sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddc2af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
